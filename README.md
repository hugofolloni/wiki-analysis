# wiki-analysis
Projeto final de √Ålgebra Linear Algor√≠tmica na turma de 2022.1 da UFRJ, tem como intuito responder a categoria de um artigo com base na sua url e outros artigos semelhantes, por meio de compara√ß√µes usando √°lgebra linear. 

- [üí° Ideia do projeto](#ideia-do-projeto)
- [üßÆ Algoritmo](#algoritmo)
- [üî¥ Live](#live)
- [üèóÔ∏è Estrutura](#estrutura)
- [ü™õ Funcionamento](#funcionamento)
- [üñ•Ô∏è Tecnologias](#tecnologias)
- [üìì Como rodar](#como-rodar)
- [‚õî Erro inicial](#erro-inicial)


## üí° Ideia do Projeto
O projeto se baseou na tentativa de categorizar um artigo da Wikip√©dia com base em outros artigos, encontrando vetores para cada um deles. Utilizando-se de √Ålgebra Linear, calcula a similaridade entre as p√°ginas, utilizando o cosseno entre o vetor da p√°gina e cada um dos vetores do banco de dados. Al√©m disso, o algoritmo √© capaz de aprender com as requisi√ß√µes, se tornando mais eficiente.

## üßÆ Algoritmo 

O algoritmo construir√°, para cada artigo da Wikip√©dia, um vetor, levando em conta as palavras formadoras de cada dimens√£o. Ent√£o, ele far√° a compara√ß√£o da p√°gina com base nos cossenos entre ela e os outros vetores.

##### F√≥rmula do Cosseno 
$cos(x, y) = {x^t y \over \||x|| ||y||}$


O cosseno varia entre [-1, 1] e, quanto maior, mais semelhantes dois vetores s√£o. Essa foi a estrat√©gia escolhida para comparar nossas p√°ginas, e determinar sua categoria com base nas mais semelhantes.

A cada requisi√ß√£o, o algoritmo salva o novo vetor no banco de dados, e, com isso, aprende e aumenta o n√∫mero de compara√ß√µes que ele pode fazer, o que aumenta a efici√™ncia.

## üî¥ Live 
O projeto est√° funcionando sob o [link](https://wiki-analysis.netlify.app/) e possui tamb√©m uma [API](https://wiki-analysis-ala.herokuapp.com/api/).

Um PDF com o relat√≥rio que cont√©m explica√ß√µes mais detalhadas do projeto est√° em: [PDF](https://github.com/hugofolloni/wiki-analysis/blob/master/public/relat√≥rio.pdf)


## üèóÔ∏è Estrutura 
### Front-end 
Feito em Typescript + React, possui dois componentes principais:
- Analysis: respons√°vel pela maioria das fun√ß√µes, recebe uma p√°gina do usu√°rio, faz o scrappy, gera o vetor e compara com os vetores do banco de dados. Ap√≥s isso, faz a requisi√ß√£o para o back-end, salvando a nova p√°gina.
- Admin: respons√°vel por apagar itens do banco de dados em caso de erros.
### Back-end
Feito em Typescript-Node + Express + Python, possui duas partes principais:
#### Setup (Python)
Gera os dados necess√°rios para o funcionamento da aplica√ß√£o, antes de qualquer requisi√ß√£o do usu√°rio.
- categories: possui as categorias analisadas na cria√ß√£o do vetor.
    - /pages: possui as p√°ginas que foram utilizadas para criar o vetor.
    - /words: possui a lista de palavras encontradas nas p√°ginas utilizadas, ordenadas por frequ√™ncia.
    - vector: possui o vetor final encontrado para ser utilizado pelo algoritmo.
- scrapper: faz o scrappy das p√°ginas-base.
- comparision: faz a compara√ß√£o da p√°gina que queremos analisar com as outras do banco de dados.
- words_frequency: faz a contagem da ocorr√™ncia de cada palavra buscando criar o vetor ideal.
- vector: determina quais palavras valem a pena serem usadas no vector.
- population: popula o banco de dados com as p√°ginas definidas em categories.
- reliability: indica a porcentagem de confiabilidade do algoritmo com base nas p√°ginas-base.
#### API (Typescript)
Respons√°vel por enviar os dados para o front-end, e receber as requisi√ß√µes tamb√©m.
API gerada com Express e Typescript, cria o banco de dados, fornece os endpoints para a API e faz as requisi√ß√µes para o banco (tanto post, quanto get, delete e put).

## ü™õ Funcionamento
### Prepara√ß√£o
Todos os processos aqui s√£o realizados em Python, dentro da pasta /server/src/setup.
- O administrador define artigos para cada uma das categorias dentro da pasta /categorias/pages  .
    - O preferencial √© que haja ao m√≠nimo 50 artigos de cada categoria.
- Ap√≥s isso, o scrapper.py faz o scrappy dos artigos e o words_frequency.py encontra as palavras mais comuns de cada categoria, e as escreve em /categorias/pages/words.
    - Ignorando as palavras definidas em generic.py.
- O algoritmo de compara√ß√£o em vector.py ent√£o √© rodado, definindo para cada categoria, quais palavras s√£o comuns apenas nela.
    - Ent√£o s√£o escolhidas as 50 palavras √∫nicas de categoria mais comuns, ap√≥s fazer uma limpeza nas palavras pouco relevantes.
- O usu√°rio roda o server (arquivo /server/src/index.ts), gerando o banco de dados.
- √â rodado o population, buscando popular o banco de dados com as p√°ginas j√° escolhidas, definindo a categoria delas com base nas palavras escolhidas para o vetor.
- Ao fim da prepara√ß√£o, √© rodado o reliability, que calcula a porcentagem de confiabilidade do algoritmo. Caso seja abaixo de 90%, √© prefer√≠vel refazer a escolha de palavras para o vetor.

### Execu√ß√£o
- O usu√°rio liga o servidor, rodando o index.ts dentro de /server/src, ganhando acesso ao banco de dados por meio da API.
- O usu√°rio liga o front-end, rodando yarn start na pasta root, subindo o servidor React no localhost.
- O usu√°rio acessa a p√°gina inicial e indica qual p√°gina deseja ser analisada.
- O algoritmo presente em /src/pages/Analysis.tsx ir√°:
    - Fazer um scrappy do artigo indicado, contando a ocorr√™ncia de cada uma das palavras definidas em /server/src/setup/categories/vector.txt e montando ent√£o o vetor da palavra.
    - O algoritmo ir√° requisitar da API os vetores para todas as p√°ginas do banco de dados, ganhando tamb√©m seus nomes e suas categorias.
    - O algoritmo ir√° calcular tanto a dist√¢ncia quanto o cosseno entre o vetor da p√°gina indicada e todos os do banco de dados, ordenando ent√£o por maior cosseno (mais similar) e menor dist√¢ncia (tamb√©m mais similar).
    - Como foi escolhido o m√©todo do cosseno, o algoritmo ir√° receber os 5 maiores cossenos e criar uma lista com todas as categorias de cada uma das p√°ginas. Caso toda a lista seja igual, ent√£o √© definida apenas uma categoria. Se houver mais de uma categoria, ent√£o s√£o definidas categoria prinicipal e secund√°ria.
    - Ser√° salvo ent√£o no banco de dados o nome, vetor, categoria e url da p√°gina solicitada, para melhoria da aplica√ß√£o.
    - Ap√≥s isso, o algoritmo ir√° enviar os dados para o front-end, que ir√° montar a p√°gina de resultado, mostrando a(s) categoria(s) e recomendar 5 artigos com maior cosseno (mais pr√≥ximos) quando comparados ao artigo solicitado.
- Por meio do algoritmo presente em /src/pages/Admin.tsx, o usu√°rio pode apagar dados incorretos do banco de dados, passando o ID no banco de dados.

## üíª Tecnologias
- React
- TypeScript
- Python
- NodeJS
- SQLite
- Express
- FS
- Numpy
- Vectorius
- BeautifulSoup
- Axios


## üìì Como rodar:
```bash
# Clone o reposit√≥rio 
$ git clone https://github.com/hugofolloni/wiki-analysis

# Acesse a pasta
$ cd wiki-analysis

# Instale os pacotes necess√°rios
$ yarn
$ cd server 
$ yarn 
$ cd ..

# Rode o servidor, para criar o banco de dados
$ yarn server

# Crie a pasta /pages dentro de /server/src/setup/categories e defina as p√°ginas que o algoritmo utilizar√° para aprender

# Acesse a pasta de setup e rode o arquivo vector.py
## Este arquivo roda todas as fun√ß√µes necess√°rias para defini√ß√£o do vetor
$ cd server/src/setup
$ python vector.py

# Volte ao root e rode o front-end
$ cd ../../../
$ yarn start

```
## ‚õî Erro inicial
Inicialmente, para o banco de dados rec√©m-criado, houve uma taxa de acerto de categorias de 91.2%, para 575 p√°ginas. 
