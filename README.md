# wiki-analysis
Projeto final de Ãlgebra Linear AlgorÃ­tmica na turma de 2022.1 da UFRJ, tem como intuito responder a categoria de um artigo com base na sua url e outros artigos semelhantes, por meio de comparaÃ§Ãµes usando Ã¡lgebra linear. 

- [ğŸ’¡ Ideia do projeto](#ğŸ’¡-ideia-do-projeto)
- [ğŸ§® Algoritmo](#ğŸ§®-algoritmo)
- [ğŸ”´ Live](#ğŸ”´-live)
- [ğŸ—ï¸ Estrutura](#ğŸ—ï¸-estrutura)
- [ğŸª› Funcionamento](#ğŸª›-funcionamento)
- [ğŸ–¥ï¸ Tecnologias](#ğŸ–¥ï¸-tecnologias)
- [ğŸ““ Como rodar](#ğŸ““-como-rodar)
- [â›” Erro inicial](#â›”-erro-inicial)


## ğŸ’¡ Ideia do Projeto
O projeto se baseou na tentativa de categorizar um artigo da WikipÃ©dia com base em outros artigos, encontrando vetores para cada um deles. Utilizando-se de Ãlgebra Linear, calcula a similaridade entre as pÃ¡ginas, utilizando o cosseno entre o vetor da pÃ¡gina e cada um dos vetores do banco de dados. AlÃ©m disso, o algoritmo Ã© capaz de aprender com as requisiÃ§Ãµes, se tornando mais eficiente.

## ğŸ§® Algoritmo 

O algoritmo construirÃ¡, para cada artigo da WikipÃ©dia, um vetor, levando em conta as palavras formadoras de cada dimensÃ£o. EntÃ£o, ele farÃ¡ a comparaÃ§Ã£o da pÃ¡gina com base nos cossenos entre ela e os outros vetores.

##### FÃ³rmula do Cosseno 
$cos(x, y) = {x^t y \over \||x|| ||y||}$


O cosseno varia entre [-1, 1] e, quanto maior, mais semelhantes dois vetores sÃ£o. Essa foi a estratÃ©gia escolhida para comparar nossas pÃ¡ginas, e determinar sua categoria com base nas mais semelhantes.

A cada requisiÃ§Ã£o, o algoritmo salva o novo vetor no banco de dados, e, com isso, aprende e aumenta o nÃºmero de comparaÃ§Ãµes que ele pode fazer, o que aumenta a eficiÃªncia.

## ğŸ”´ Live 
O projeto estÃ¡ funcionando sob o [link](https://wiki-analysis.netlify.app/) e possui tambÃ©m uma [API](https://wiki-analysis-ala.herokuapp.com/api/).

Um PDF com o relatÃ³rio que contÃ©m explicaÃ§Ãµes mais detalhadas do projeto estÃ¡ em: [PDF](https://github.com/hugofolloni/wiki-analysis/blob/master/public/relatÃ³rio.pdf)


## ğŸ—ï¸ Estrutura 
### Front-end 
Feito em Typescript + React, possui dois componentes principais:
- Analysis: responsÃ¡vel pela maioria das funÃ§Ãµes, recebe uma pÃ¡gina do usuÃ¡rio, faz o scrappy, gera o vetor e compara com os vetores do banco de dados. ApÃ³s isso, faz a requisiÃ§Ã£o para o back-end, salvando a nova pÃ¡gina.
- Admin: responsÃ¡vel por apagar itens do banco de dados em caso de erros.
### Back-end
Feito em Typescript-Node + Express + Python, possui duas partes principais:
#### Setup (Python)
Gera os dados necessÃ¡rios para o funcionamento da aplicaÃ§Ã£o, antes de qualquer requisiÃ§Ã£o do usuÃ¡rio.
- categories: possui as categorias analisadas na criaÃ§Ã£o do vetor.
    - /pages: possui as pÃ¡ginas que foram utilizadas para criar o vetor.
    - /words: possui a lista de palavras encontradas nas pÃ¡ginas utilizadas, ordenadas por frequÃªncia.
    - vector: possui o vetor final encontrado para ser utilizado pelo algoritmo.
- scrapper: faz o scrappy das pÃ¡ginas-base.
- comparision: faz a comparaÃ§Ã£o da pÃ¡gina que queremos analisar com as outras do banco de dados.
- words_frequency: faz a contagem da ocorrÃªncia de cada palavra buscando criar o vetor ideal.
- vector: determina quais palavras valem a pena serem usadas no vector.
- population: popula o banco de dados com as pÃ¡ginas definidas em categories.
- reliability: indica a porcentagem de confiabilidade do algoritmo com base nas pÃ¡ginas-base.
#### API (Typescript)
ResponsÃ¡vel por enviar os dados para o front-end, e receber as requisiÃ§Ãµes tambÃ©m.
API gerada com Express e Typescript, cria o banco de dados, fornece os endpoints para a API e faz as requisiÃ§Ãµes para o banco (tanto post, quanto get, delete e put).

## ğŸª› Funcionamento
### PreparaÃ§Ã£o
Todos os processos aqui sÃ£o realizados em Python, dentro da pasta /server/src/setup.
- O administrador define artigos para cada uma das categorias dentro da pasta /categorias/pages  .
    - O preferencial Ã© que haja ao mÃ­nimo 50 artigos de cada categoria.
- ApÃ³s isso, o scrapper.py faz o scrappy dos artigos e o words_frequency.py encontra as palavras mais comuns de cada categoria, e as escreve em /categorias/pages/words.
    - Ignorando as palavras definidas em generic.py.
- O algoritmo de comparaÃ§Ã£o em vector.py entÃ£o Ã© rodado, definindo para cada categoria, quais palavras sÃ£o comuns apenas nela.
    - EntÃ£o sÃ£o escolhidas as 50 palavras Ãºnicas de categoria mais comuns, apÃ³s fazer uma limpeza nas palavras pouco relevantes.
- O usuÃ¡rio roda o server (arquivo /server/src/index.ts), gerando o banco de dados.
- Ã‰ rodado o population, buscando popular o banco de dados com as pÃ¡ginas jÃ¡ escolhidas, definindo a categoria delas com base nas palavras escolhidas para o vetor.
- Ao fim da preparaÃ§Ã£o, Ã© rodado o reliability, que calcula a porcentagem de confiabilidade do algoritmo. Caso seja abaixo de 90%, Ã© preferÃ­vel refazer a escolha de palavras para o vetor.

### ExecuÃ§Ã£o
- O usuÃ¡rio liga o servidor, rodando o index.ts dentro de /server/src, ganhando acesso ao banco de dados por meio da API.
- O usuÃ¡rio liga o front-end, rodando yarn start na pasta root, subindo o servidor React no localhost.
- O usuÃ¡rio acessa a pÃ¡gina inicial e indica qual pÃ¡gina deseja ser analisada.
- O algoritmo presente em /src/pages/Analysis.tsx irÃ¡:
    - Fazer um scrappy do artigo indicado, contando a ocorrÃªncia de cada uma das palavras definidas em /server/src/setup/categories/vector.txt e montando entÃ£o o vetor da palavra.
    - O algoritmo irÃ¡ requisitar da API os vetores para todas as pÃ¡ginas do banco de dados, ganhando tambÃ©m seus nomes e suas categorias.
    - O algoritmo irÃ¡ calcular tanto a distÃ¢ncia quanto o cosseno entre o vetor da pÃ¡gina indicada e todos os do banco de dados, ordenando entÃ£o por maior cosseno (mais similar) e menor distÃ¢ncia (tambÃ©m mais similar).
    - Como foi escolhido o mÃ©todo do cosseno, o algoritmo irÃ¡ receber os 5 maiores cossenos e criar uma lista com todas as categorias de cada uma das pÃ¡ginas. Caso toda a lista seja igual, entÃ£o Ã© definida apenas uma categoria. Se houver mais de uma categoria, entÃ£o sÃ£o definidas categoria prinicipal e secundÃ¡ria.
    - SerÃ¡ salvo entÃ£o no banco de dados o nome, vetor, categoria e url da pÃ¡gina solicitada, para melhoria da aplicaÃ§Ã£o.
    - ApÃ³s isso, o algoritmo irÃ¡ enviar os dados para o front-end, que irÃ¡ montar a pÃ¡gina de resultado, mostrando a(s) categoria(s) e recomendar 5 artigos com maior cosseno (mais prÃ³ximos) quando comparados ao artigo solicitado.
- Por meio do algoritmo presente em /src/pages/Admin.tsx, o usuÃ¡rio pode apagar dados incorretos do banco de dados, passando o ID no banco de dados.

## ğŸ’» Tecnologias
- React
- TypeScript
- Python
- NodeJS
- SQLite
- Express
- FS
- Numpy
- Vectorius
- BeautifulSoup
- Axios


## ğŸ““ Como rodar:
```bash
# Clone o repositÃ³rio 
$ git clone https://github.com/hugofolloni/wiki-analysis

# Acesse a pasta
$ cd wiki-analysis

# Instale os pacotes necessÃ¡rios
$ yarn
$ cd server 
$ yarn 
$ cd ..

# Rode o servidor, para criar o banco de dados
$ yarn server

# Crie a pasta /pages dentro de /server/src/setup/categories e defina as pÃ¡ginas que o algoritmo utilizarÃ¡ para aprender

# Acesse a pasta de setup e rode o arquivo vector.py
## Este arquivo roda todas as funÃ§Ãµes necessÃ¡rias para definiÃ§Ã£o do vetor
$ cd server/src/setup
$ python vector.py

# Volte ao root e rode o front-end
$ cd ../../../
$ yarn start

```
## â›” Erro inicial
Inicialmente, para o banco de dados recÃ©m-criado, houve uma taxa de acerto de categorias de 91.2%, para 575 pÃ¡ginas. 
